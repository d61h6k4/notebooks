{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers from scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d61h6k4/notebooks/blob/master/Transformers_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNDP2Bi77inZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "outputId": "50adf581-d043-421c-ddf7-4eaf87e1b2e8"
      },
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 1.2MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2 (from tensorflow)\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 33.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.5)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 26.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (41.2.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=dbfa58d6c05bbbb6e0daac211cf1e3fde42ebfe19c380fee7b46462f3194aba0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: -ensorflow 1.14.0 has requirement tensorboard<1.15.0,>=1.14.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: -ensorflow 1.14.0 has requirement tensorflow-estimator<1.15.0rc0,>=1.14.0rc0, but you'll have tensorflow-estimator 2.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.2\n",
            "    Uninstalling gast-0.3.2:\n",
            "      Successfully uninstalled gast-0.3.2\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "Successfully installed gast-0.2.2 tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLDTC_Ls-JvH",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we will follow Peter Bloem's blogpost [Transformers from scratch](http://www.peterbloem.nl/blog/transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxvC4wksLRlk",
        "colab_type": "text"
      },
      "source": [
        "To better understand let's go step by step:\n",
        "1. we have keys, queries and values:\n",
        "$$\n",
        "q_i = W_q x_i \\quad k_i = W_k x_i \\quad v_i = W_v x_i \\\\\n",
        "w'_{ij} = q_i^T k_j \\\\\n",
        "w_{i,j} = softmax(w'_{ij}) \\\\\n",
        "y_i = \\sum_j w_{i,j}v_j\n",
        "$$\n",
        "2. Scaling the dot product:\n",
        "$$\n",
        "q_i = W_q x_i \\quad k_i = W_k x_i \\quad v_i = W_v x_i \\\\\n",
        "w'_{ij} = \\frac{q_i^T k_j}{\\sqrt{k}} \\\\\n",
        "w_{i,j} = softmax(w'_{ij}) \\\\\n",
        "y_i = \\sum_j w_{i,j}v_j\n",
        "$$\n",
        "3. Multi-head attention (and here we made trick with storing all heads in one weight)\n",
        "$$\n",
        "q_i = W^r_q x_i \\quad k_i = W^r_k x_i \\quad v_i = W^r_v x_i \\\\\n",
        "w'_{ij} = \\frac{q_i^T k_j}{\\sqrt{k}} \\\\\n",
        "w_{i,j} = softmax(w'_{ij}) \\\\\n",
        "y_i = \\sum_j w_{i,j}v_j\n",
        "$$\n",
        "\n",
        "So, let's write dimensions of operations to understand what will exactly happens: \\\\\n",
        "$X$ - input will be from $\\mathbb{R}^{b \\times t \\times k}$, where\n",
        "$b$ is batch size, $t$ is size of input sentence and $k$ is size of word vector.\n",
        "\n",
        "$W_q \\in \\mathbb{R}^{h * k \\times k}$ - $h$ times concatenated $W_q$ from 1, 2 steps. \\\\\n",
        "$q = X W_q \\in \\mathbb{R}^{b \\times t \\times h*k}$ - we will implement it in this way. \\\\\n",
        "$k = X W_q \\in \\mathbb{R}^{b \\times t \\times h*k}$.\n",
        "\n",
        "So, the next operation is $w'_{ij} = \\frac{q_i^Tk_j}{\\sqrt{k}}$, and we will process it in next few steps:\n",
        "1. reshape $q$ and $k$ ($b \\times t \\times h*k \\rightarrow b \\times t \\times h \\times k$)\n",
        "2. transpose (you can think about transpose as interchanging dimensions, so we get next $b \\times t \\times h \\times k \\rightarrow b \\times h \\times t \\times k$)\n",
        "3. reshape ($b \\times h \\times t \\times k \\rightarrow b*h \\times t \\times k$) and now we can thin about $q$ and $k$ as $h$ times batches of original (not multi-headed) $q$ and $k$ correspondingly.\n",
        "4. from computation efficency perspective it's better devide on scalar ($\\sqrt{k}$) before scalar product, thus we devide on $\\sqrt[4]{k}$ each argument $q$ and $k$ . (we devide on $\\sqrt[4]{k}$ cause $\\sqrt[4]{k} * \\sqrt[4]{k} = \\sqrt{k}$)\n",
        "5. Now we multiply $q$ and $k^T$ in batch matrix multiplaction (matrix multiplication which didn't count batch dimension) and got $w' \\in \\mathbb{R}^{b*h \\times t \\times t}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRLyZ5f8c8rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6xSj8A3714H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, heads=8):\n",
        "        super().__init__()\n",
        "        self.__heads = heads\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # k as in original blog \n",
        "        k = input_shape[-1]\n",
        "        # These compute the queries, keys and values for all\n",
        "        # heads (as a single concatenated vectors)\n",
        "        self.tokeys = tf.keras.layers.Dense(k * self.__heads, activation=\"linear\", use_bias=False, input_dim=(k,))\n",
        "        self.toqueries = tf.keras.layers.Dense(k * self.__heads, activation=\"linear\", use_bias=False, input_dim=(k,))\n",
        "        self.tovalues = tf.keras.layers.Dense(k * self.__heads, activation=\"linear\", use_bias=False, input_dim=(k,))\n",
        "\n",
        "        # This unifies the outputs of the different heads into\n",
        "        # a single k-vector\n",
        "        self.unifyheads = tf.keras.layers.Dense(k, activation=\"linear\", input_dim=(k * self.__heads))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        b, t, k = tf.shape(inputs)\n",
        "        h  = self.__heads\n",
        "\n",
        "        queries = tf.reshape(self.toqueries(inputs), [b, t, h, k])\n",
        "        keys = tf.reshape(self.tokeys(inputs), [b, t, h, k])\n",
        "        values = tf.reshape(self.tovalues(inputs), [b, t, h, k])\n",
        "\n",
        "        queries = tf.transpose(queries, perm=[0, 2, 1, 3])\n",
        "        keys = tf.transpose(keys, perm=[0, 2, 1, 3])\n",
        "        values = tf.transpose(values, perm[0, 2, 1, 3])\n",
        "\n",
        "        queries = tf.reshape(queries, [b * h, t, k])\n",
        "        keys = tf.reshape(keys, [b * h, t, k])\n",
        "        values = tf.reshape(values, [b * h, t, k])\n",
        "\n",
        "        queries = queries / (k ** (1 / 4))\n",
        "        keys = keys / (k ** (1 / 4))\n",
        "\n",
        "        dot = tf.linalg.matmul(queries, keys, transpose_b=True)\n",
        "        dot = tf.math.softmax(dot, axis=2)\n",
        "\n",
        "        out = tf.linalg.matmul(dot, values)\n",
        "        out = tf.reshape(out, [b, h, t, k])\n",
        "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
        "        out = tf.reshape(out, [b, t, h * k])\n",
        "\n",
        "        return self.unifyheads(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0pJrLCMcngd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}