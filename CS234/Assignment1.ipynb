{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d61h6k4/notebooks/blob/master/CS234/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdclU0GTdmTe",
        "colab_type": "code",
        "outputId": "06ba71a2-6636-4591-afa2-bcd57285f212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!pip install --upgrade gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gym in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojf-GrIEebjr",
        "colab_type": "text"
      },
      "source": [
        "### MDP Value Iteration and Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5d_5h_edtW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhLgS_sJe1Tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.set_printoptions(precision=3)\n",
        "\n",
        "gym.envs.registration.register(\n",
        "    id='Deterministic-4x4-FrozenLake-v0',\n",
        "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "    kwargs={'map_name': '4x4',\n",
        "            'is_slippery': False})\n",
        "\n",
        "gym.envs.registration.register(\n",
        "    id='Deterministic-8x8-FrozenLake-v0',\n",
        "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "    kwargs={'map_name': '8x8',\n",
        "            'is_slippery': False})\n",
        "\n",
        "gym.envs.registration.register(\n",
        "    id='Stochastic-4x4-FrozenLake-v0',\n",
        "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "    kwargs={'map_name': '4x4',\n",
        "            'is_slippery': True})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4KoMBL-e8j-",
        "colab_type": "text"
      },
      "source": [
        "For policy_evaluation, policy_improvement, policy_iteration and value_iteration,\n",
        "the parameters P, nS, nA, gamma are defined as follows:\n",
        "\n",
        "\tP: nested dictionary\n",
        "\t\tFrom gym.core.Environment\n",
        "\t\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
        "\t\ttuple of the form (probability, nextstate, reward, terminal) where\n",
        "\t\t\t- probability: float\n",
        "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
        "\t\t\t- nextstate: int\n",
        "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
        "\t\t\t- reward: int\n",
        "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
        "\t\t\t\t\"nextstate\" with \"action\"\n",
        "\t\t\t- terminal: bool\n",
        "\t\t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
        "\tnS: int\n",
        "\t\tnumber of states in the environment\n",
        "\tnA: int\n",
        "\t\tnumber of actions in the environment\n",
        "\tgamma: float\n",
        "\t\tDiscount factor. Number in range [0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfdq0aqge8Lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
        "\t\"\"\"Evaluate the value function from a given policy.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\tpolicy: np.array[nS]\n",
        "\t\tThe policy to evaluate. Maps states to actions.\n",
        "\ttol: float\n",
        "\t\tTerminate policy evaluation when\n",
        "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
        "\tReturns\n",
        "\t-------\n",
        "\tvalue_function: np.ndarray[nS]\n",
        "\t\tThe value function of the given policy, where value_function[s] is\n",
        "\t\tthe value of state s\n",
        "\t\"\"\"\n",
        "\n",
        "\tvalue_function = np.zeros(nS)\n",
        "\n",
        "\t############################\n",
        "\t# YOUR IMPLEMENTATION HERE #\n",
        "\tprev_value_function = np.ones(nS)\n",
        "\tvalue_function = np.zeros(nS)\n",
        " \n",
        "\twhile np.max(np.abs(value_function - prev_value_function)) > tol:\n",
        "\t\tprev_value_function = np.copy(value_function)\n",
        "\t\tfor s in range(nS):\n",
        "\t\t\tprobability, nextstate, reward, terminal = P[s][policy[s]][0]\n",
        "\t\t\tvalue_function[s] = reward + gamma * probability * prev_value_function[nextstate]\n",
        "\n",
        "\t############################\n",
        "\treturn value_function\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPn0dKBBe6mO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
        "\t\"\"\"Given the value function from policy improve the policy.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\tvalue_from_policy: np.ndarray\n",
        "\t\tThe value calculated from the policy\n",
        "\tpolicy: np.array\n",
        "\t\tThe previous policy.\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tnew_policy: np.ndarray[nS]\n",
        "\t\tAn array of integers. Each integer is the optimal action to take\n",
        "\t\tin that state according to the environment dynamics and the\n",
        "\t\tgiven value function.\n",
        "\t\"\"\"\n",
        "\n",
        "\tnew_policy = np.zeros(nS, dtype='int')\n",
        "\n",
        "\t############################\n",
        "\t# YOUR IMPLEMENTATION HERE #\n",
        "\tstate_action_value = np.zeros((nS, nA))\n",
        "\tfor s in range(nS):\n",
        "\t\tfor a in range(nA):\n",
        "\t\t\tprobability, nextstate, reward, terminal = P[s][a][0]\n",
        "\t\t\tstate_action_value[s][a] = reward + gamma * probability * value_from_policy[nextstate]\n",
        "\tnew_policy = np.argmax(state_action_value, axis=1)\n",
        "\t############################\n",
        "\treturn new_policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9uHzAhLfUx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
        "\t\"\"\"Runs policy iteration.\n",
        "\n",
        "\tYou should call the policy_evaluation() and policy_improvement() methods to\n",
        "\timplement this method.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\ttol: float\n",
        "\t\ttol parameter used in policy_evaluation()\n",
        "\tReturns:\n",
        "\t----------\n",
        "\tvalue_function: np.ndarray[nS]\n",
        "\tpolicy: np.ndarray[nS]\n",
        "\t\"\"\"\n",
        "\tvalue_function = np.zeros(nS)\n",
        "\tpolicy = np.zeros(nS, dtype=int)\n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "\tpolicy = np.zeros(nS)\n",
        "\tprev_policy = np.ones(nS)\n",
        "\twhile np.max(np.abs(policy - prev_policy)) > tol:\n",
        "\t\tprev_policy = np.copy(policy)\n",
        "\t\tvalue_function = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
        "\t\tpolicy = policy_improvement(P, nS, nA, value_function, policy, gamma)\n",
        "\t############################\n",
        "\treturn value_function, policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5z6aVdSfWsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
        "\t\"\"\"\n",
        "\tLearn value function and policy by using value iteration method for a given\n",
        "\tgamma and environment.\n",
        "\n",
        "\tParameters:\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\ttol: float\n",
        "\t\tTerminate value iteration when\n",
        "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
        "\tReturns:\n",
        "\t----------\n",
        "\tvalue_function: np.ndarray[nS]\n",
        "\tpolicy: np.ndarray[nS]\n",
        "\t\"\"\"\n",
        "\n",
        "\tvalue_function = np.zeros(nS)\n",
        "\tpolicy = np.zeros(nS, dtype=int)\n",
        "\t############################\n",
        "\t# YOUR IMPLEMENTATION HERE #\n",
        "\tprev_value_function = np.ones(nS)\n",
        "\twhile np.max(np.abs(value_function - prev_value_function)) > tol:\n",
        "\t\tprev_value_function = np.copy(value_function)\n",
        "\t\tstate_action_value = np.zeros((nS, nA))\n",
        "\t\tfor s in range(nS):\n",
        "\t\t\tfor a in range(nA):\n",
        "\t\t\t\tprobability, nextstate, reward, terminal = P[s][a][0]\n",
        "\t\t\t\tstate_action_value[s][a] = reward + gamma * probability * value_function[nextstate]\n",
        "\t\tvalue_function = np.max(state_action_value, axis=1)\n",
        "\tpolicy = np.argmax(state_action_value, axis=1)\n",
        "\n",
        "\t############################\n",
        "\treturn value_function, policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htGhwcw6faZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def render_single(env, policy, max_steps=100):\n",
        "  \"\"\"\n",
        "    This function does not need to be modified\n",
        "    Renders policy once on environment. Watch your agent play!\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      Environment to play on. Must have nS, nA, and P as\n",
        "      attributes.\n",
        "    Policy: np.array of shape [env.nS]\n",
        "      The action to take at a given state\n",
        "  \"\"\"\n",
        "\n",
        "  episode_reward = 0\n",
        "  ob = env.reset()\n",
        "  for t in range(max_steps):\n",
        "    env.render()\n",
        "    time.sleep(0.25)\n",
        "    a = policy[ob]\n",
        "    ob, rew, done, _ = env.step(a)\n",
        "    episode_reward += rew\n",
        "    if done:\n",
        "      break\n",
        "  env.render();\n",
        "  if not done:\n",
        "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
        "  else:\n",
        "  \tprint(\"Episode reward: %f\" % episode_reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_b4gQjad-G-",
        "colab_type": "code",
        "outputId": "7b8fdc5b-61af-4686-b40e-345e8fc5d31a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# comment/uncomment these lines to switch between deterministic/stochastic environments\n",
        "# env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
        "env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
        "\n",
        "V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
        "print(f\"Policy iteration: {p_pi}\")\n",
        "render_single(env, p_pi, 1000)\n",
        "\n",
        "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
        "\n",
        "V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
        "print(f\"Value iteration: {p_vi}\")\n",
        "render_single(env, p_vi, 1000)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------------\n",
            "Beginning Policy Iteration\n",
            "-------------------------\n",
            "Policy iteration: [2 3 2 1 2 0 2 0 3 2 2 0 0 3 3 0]\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Episode reward: 0.000000\n",
            "\n",
            "-------------------------\n",
            "Beginning Value Iteration\n",
            "-------------------------\n",
            "Value iteration: [2 3 2 1 2 0 2 0 3 2 2 0 0 3 3 0]\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHF\u001b[41mH\u001b[0m\n",
            "FFFH\n",
            "HFFG\n",
            "Episode reward: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwQFBos3m2bb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}